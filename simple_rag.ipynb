{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armansarder/SauceBros/blob/main/simple_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Et1ikJmoAe"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "## Scan the QR code or access the tinyurl link to open the colab and make a copy of it to follow along!\n",
        "\n",
        "File > Save a Copy in Drive\n",
        "\n",
        "![simple-rag-qr.png](data:image/png;base64,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)\n",
        "\n",
        "https://tinyurl.com/simple-rag\n",
        "\n",
        "# Simple RAG Workshop with LangChain and Gemini API\n",
        "\n",
        "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) system using:\n",
        "- **LangChain** for RAG pipeline orchestration\n",
        "- **Gemini API** for text generation\n",
        "- **PDF documents** from a data directory\n",
        "- **Vector embeddings** for semantic search\n",
        "- **ChromaDB** for vector storage and retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vxmyZEQNmoAg"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run this cell if packages are not installed)\n",
        "!pip install -q langchain langchain-google-genai langchain-community langchain-core langchain-text-splitters python-dotenv pypdf chromadb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following api key: AIzaSyAFMkRU2Zy28LxA4XWXzXeuxbAbejReZ9Q. To create a secret in google colab:\n",
        "1. Click on the key icon in the sidebar of the notebook\n",
        "2. Add new secret\n",
        "3. Type in gemini for the name and the value: AIzaSyAFMkRU2Zy28LxA4XWXzXeuxbAbejReZ9Q.\n",
        "4. Enable notebook access for the secret\n",
        "\n",
        "Note: The api key will be destroyed after this workshop, if you'd like to play with these scripts later, visit: https://aistudio.google.com for your own api key!"
      ],
      "metadata": {
        "id": "709VijbWOYwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AjfCoiQmoAi"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import glob\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load environment variables\n",
        "\n",
        "# Verify API key is loaded\n",
        "GEMINI_API_KEY = userdata.get('gemini')\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
        "\n",
        "# Also set as GOOGLE_API_KEY for LangChain compatibility\n",
        "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n",
        "\n",
        "print(\"Environment variables loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VULvRgmWmoAi"
      },
      "source": [
        "## Step 1: Load PDF Documents\n",
        "\n",
        "This step prepares the document collection for the RAG system:\n",
        "\n",
        "- **Download documents from Google Drive**: Authenticate with Google Drive API and copy the shared folder containing PDF documents to your Drive\n",
        "- **Locate PDF files**: Search for all PDF files in the specified data directory\n",
        "- **Load PDF documents**: Use PyPDFLoader to extract text content from each PDF file\n",
        "- **Add metadata**: Attach source file information to each document for traceability\n",
        "- **Display summary**: Show the total number of documents loaded and total character count across all pages\n",
        "\n",
        "Replace the text at ```shared_link = \"INSERT LINK HERE\"``` with \"https://drive.google.com/drive/folders/1iwWlvtMj-QY0usHFQGqPGDv1Vu9Y7yaq\". This is how we copy the 'data' folder from someone else's drive into our own.\n",
        "\n",
        "Make sure you allow all permissions for your drive during the mounting authentication for this to work."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydrive2\n",
        "\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from googleapiclient.discovery import build\n",
        "import re\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "service = build(\"drive\", \"v3\", credentials=gauth.credentials)\n",
        "\n",
        "def extract_id_from_url(url: str) -> str:\n",
        "    patterns = [\n",
        "        r\"/d/([a-zA-Z0-9_-]+)\",\n",
        "        r\"id=([a-zA-Z0-9_-]+)\",\n",
        "        r\"/folders/([a-zA-Z0-9_-]+)\",\n",
        "    ]\n",
        "    for p in patterns:\n",
        "        m = re.search(p, url)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    return url.strip()\n",
        "\n",
        "def copy_file(service, file_id, parent_id=None, new_name=None):\n",
        "    body = {}\n",
        "    if new_name:\n",
        "        body[\"name\"] = new_name\n",
        "    if parent_id:\n",
        "        body[\"parents\"] = [parent_id]\n",
        "\n",
        "    copied = service.files().copy(\n",
        "        fileId=file_id,\n",
        "        body=body,\n",
        "        fields=\"id, name\"\n",
        "    ).execute()\n",
        "    return copied\n",
        "\n",
        "def create_folder(service, name, parent_id=None):\n",
        "    metadata = {\n",
        "        \"name\": name,\n",
        "        \"mimeType\": \"application/vnd.google-apps.folder\"\n",
        "    }\n",
        "    if parent_id:\n",
        "        metadata[\"parents\"] = [parent_id]\n",
        "\n",
        "    folder = service.files().create(\n",
        "        body=metadata,\n",
        "        fields=\"id, name\"\n",
        "    ).execute()\n",
        "    return folder\n",
        "\n",
        "def copy_folder_recursive(service, source_folder_id, target_parent_id=None):\n",
        "    src = service.files().get(\n",
        "        fileId=source_folder_id,\n",
        "        fields=\"id, name\"\n",
        "    ).execute()\n",
        "\n",
        "    new_folder = create_folder(\n",
        "      service,\n",
        "      name=src[\"name\"],\n",
        "      parent_id=target_parent_id\n",
        "    )\n",
        "    new_folder_id = new_folder[\"id\"]\n",
        "\n",
        "    page_token = None\n",
        "    while True:\n",
        "        response = service.files().list(\n",
        "            q=f\"'{source_folder_id}' in parents and trashed=false\",\n",
        "            fields=\"nextPageToken, files(id, name, mimeType)\",\n",
        "            pageToken=page_token\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get(\"files\", []):\n",
        "            if item[\"mimeType\"] == \"application/vnd.google-apps.folder\":\n",
        "                copy_folder_recursive(service, item[\"id\"], new_folder_id)\n",
        "            else:\n",
        "                copy_file(\n",
        "                    service,\n",
        "                    file_id=item[\"id\"],\n",
        "                    parent_id=new_folder_id,\n",
        "                    new_name=item[\"name\"]\n",
        "                )\n",
        "\n",
        "        page_token = response.get(\"nextPageToken\", None)\n",
        "        if page_token is None:\n",
        "            break\n",
        "\n",
        "    return new_folder\n",
        "\n",
        "shared_link = \"https://drive.google.com/drive/folders/1iwWlvtMj-QY0usHFQGqPGDv1Vu9Y7yaq\"\n",
        "\n",
        "source_id = extract_id_from_url(shared_link)\n",
        "\n",
        "meta = service.files().get(\n",
        "    fileId=source_id,\n",
        "    fields=\"id, name, mimeType\"\n",
        ").execute()\n",
        "\n",
        "mime = meta[\"mimeType\"]\n",
        "name = meta[\"name\"]\n",
        "\n",
        "if mime == \"application/vnd.google-apps.folder\":\n",
        "    new_folder = copy_folder_recursive(service, source_id, target_parent_id=None)\n",
        "    new_link = f\"https://drive.google.com/drive/folders/{new_folder['id']}\"\n",
        "    print(f\"Copied folder: {new_link}\")\n",
        "else:\n",
        "    copied = copy_file(\n",
        "        service,\n",
        "        file_id=source_id,\n",
        "        parent_id=None,\n",
        "        new_name=name\n",
        "    )\n",
        "    new_link = f\"https://drive.google.com/file/d/{copied['id']}/view\"\n",
        "    print(f\"Copied file: {new_link} into your drive\")"
      ],
      "metadata": {
        "id": "sFZ9hknpnSC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7eeG29WmoAi"
      },
      "outputs": [],
      "source": [
        "# Load all PDF documents from the data directory (that is now inside of your drive)\n",
        "pdf_files = glob.glob(os.path.join(\"./drive/MyDrive/data\", \"*.pdf\"))\n",
        "\n",
        "if not pdf_files:\n",
        "    print(\"No PDF files found in data directory\")\n",
        "    documents = []\n",
        "else:\n",
        "    documents = []\n",
        "    for pdf_path in pdf_files:\n",
        "        try:\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            docs = loader.load()\n",
        "            # Add source metadata to each document\n",
        "            for doc in docs:\n",
        "                doc.metadata['source'] = os.path.basename(pdf_path)\n",
        "            documents.extend(docs)\n",
        "            print(f\"Loaded: {os.path.basename(pdf_path)} ({len(docs)} pages)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {pdf_path}: {e}\")\n",
        "\n",
        "print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
        "print(f\"Total pages: {sum([len(doc.page_content) for doc in documents])} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkwRQXnamoAi"
      },
      "source": [
        "## Step 2: Chunk Documents\n",
        "\n",
        "This step breaks down large documents into smaller, manageable pieces for efficient processing:\n",
        "\n",
        "- **Initialize text splitter**: Configure RecursiveCharacterTextSplitter with optimal chunk size (1000 characters) and overlap (200 characters) to maintain context between chunks\n",
        "- **Split documents**: Process all loaded documents and divide them into smaller text chunks\n",
        "- **Preserve structure**: Use separators (paragraphs, sentences, words) to split at natural boundaries\n",
        "- **Maintain metadata**: Keep source information attached to each chunk for proper attribution\n",
        "- **Display statistics**: Show total number of chunks created and average chunk size for verification\n",
        "\n",
        "Follow along with the workshop and replace the ```\"INSERT\"``` quotes with actual code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmSY5TRmmoAi"
      },
      "outputs": [],
      "source": [
        "# Split documents into chunks using LangChain's RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=\"INSERT NUMBER HERE\",  # Characters per chunk\n",
        "    chunk_overlap=\"INSERT NUMBER HERE\",  # Overlap between chunks\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split at these boundaries first\n",
        ")\n",
        "\n",
        "# Split all documents into chunks\n",
        "texts = \"INSERT TEXT SPLITTER CODE HERE\"\n",
        "\n",
        "print(f\"Total chunks created: {len(texts)}\")\n",
        "print(f\"Average chunk size: {sum([len(chunk.page_content) for chunk in texts]) / len(texts):.0f} characters\")\n",
        "print(f\"\\nSample chunk:\")\n",
        "print(f\"Source: {texts[0].metadata.get('source', 'Unknown')}\")\n",
        "print(f\"Content preview: {texts[0].page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDcI-iSfmoAj"
      },
      "source": [
        "## Step 3: Create Vector Store with ChromaDB\n",
        "\n",
        "This step converts text chunks into vector embeddings and stores them for semantic search:\n",
        "\n",
        "- **Initialize embeddings model**: Set up Google's embedding model (embedding-001) to convert text into high-dimensional vectors\n",
        "- **Generate embeddings**: Transform all document chunks into vector representations that capture semantic meaning\n",
        "- **Create vector database**: Use ChromaDB to store embeddings with their associated text chunks and metadata\n",
        "- **Persist to disk**: Save the vector store locally so it can be reused without regenerating embeddings\n",
        "- **Verify creation**: Confirm the vector store contains all document chunks and is ready for retrieval\n",
        "\n",
        "Follow along with the workshop and replace the ```\"INSERT\"``` quotes with actual code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XfYcv8BmoAj"
      },
      "outputs": [],
      "source": [
        "# Initialize embeddings using Google's embedding model\n",
        "# Explicitly pass the API key to avoid using Application Default Credentials\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/INSERT MODEL HERE\",\n",
        "    google_api_key=GEMINI_API_KEY\n",
        ")\n",
        "\n",
        "print(\"Creating vector store with ChromaDB...\")\n",
        "\n",
        "# Option 1: Create new vector store (use this the first time)\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=texts,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"  # Persist to disk for reuse\n",
        ")\n",
        "\n",
        "# Option 2: Load existing vector store (uncomment to use instead of creating new)\n",
        "# vectorstore = Chroma(\n",
        "#     persist_directory=\"./chroma_db\",\n",
        "#     embedding_function=embeddings\n",
        "# )\n",
        "\n",
        "print(f\"Vector store created with {len(texts)} documents\")\n",
        "print(f\"Database persisted to: ./chroma_db\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKPW10V2moAj"
      },
      "source": [
        "## Step 4: Initialize Gemini LLM\n",
        "\n",
        "This step sets up the language model that will generate answers based on retrieved context:\n",
        "\n",
        "- **Configure Gemini model**: Initialize the ChatGoogleGenerativeAI client with the gemini-flash-latest model\n",
        "- **Set temperature**: Configure temperature (0-1) to balance between focused and creative responses\n",
        "- **Handle system messages**: Enable conversion of system messages to human messages for compatibility\n",
        "- **Authenticate**: Provide API key for secure access to Google's Gemini API\n",
        "- **Verify initialization**: Confirm the LLM is ready to process queries and generate responses\n",
        "\n",
        "Follow along with the workshop and replace the ```\"INSERT\"``` quotes with actual code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kIz2Xn_moAj"
      },
      "outputs": [],
      "source": [
        "# Initialize Gemini LLM\n",
        "# Explicitly pass the API key to avoid using Application Default Credentials\n",
        "# gemini-flash-latest model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-flash-latest\",  # Updated to current model name\n",
        "    temperature=\"INSERT NUMBER HERE\",\n",
        "    convert_system_message_to_human=True,\n",
        "    google_api_key=GEMINI_API_KEY\n",
        ")\n",
        "\n",
        "print(\"Gemini LLM initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMG4Dj3moAj"
      },
      "source": [
        "## Step 5: Create RAG Chain\n",
        "\n",
        "This step assembles the complete RAG pipeline that combines retrieval and generation:\n",
        "\n",
        "- **Design prompt template**: Create a comprehensive prompt that instructs the LLM to answer questions using retrieved context while allowing for interpretation and synthesis\n",
        "- **Configure retriever**: Set up the vector store retriever to fetch the top 3 most relevant document chunks for each query\n",
        "- **Format documents**: Create a function to format retrieved chunks with source information for the prompt\n",
        "- **Build RAG chain**: Use LangChain Expression Language (LCEL) to chain together retrieval, formatting, prompting, and generation steps\n",
        "- **Create wrapper class**: Implement a RAGChain class that provides a clean interface matching the expected query format\n",
        "- **Verify pipeline**: Confirm the complete RAG system is ready to process queries end-to-end\n",
        "\n",
        "Follow along with the workshop and replace the ```\"INSERT\"``` quotes with actual code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwm7e9UamoAj"
      },
      "outputs": [],
      "source": [
        "# Create a custom prompt template for the RAG chain\n",
        "# This prompt allows for interpretation and reasoning while staying grounded in the documents\n",
        "prompt_template = \"\"\"You are an expert assistant helping to answer questions based on the provided context from documents.\n",
        "\n",
        "Your task is to:\n",
        "1. Use the context as the foundation for your answer\n",
        "2. Make reasonable inferences and connections between different pieces of information\n",
        "3. Synthesize and interpret the information to provide a comprehensive answer\n",
        "4. Explain concepts and provide context when helpful\n",
        "5. If the context doesn't contain enough information, clearly state what is missing\n",
        "\n",
        "Context from documents:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Instructions:\n",
        "- Base your answer on the provided context, but feel free to interpret, explain, and make logical connections\n",
        "- You can infer relationships, draw conclusions, and provide explanations that go beyond direct quotes\n",
        "- Synthesize information from multiple parts of the context when relevant\n",
        "- If the context is insufficient, explain what information would be needed to fully answer the question\n",
        "\n",
        "Provide a thoughtful, well-reasoned answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Create the RAG chain using LangChain Expression Language (LCEL)\n",
        "# This is the modern, recommended approach that works across LangChain versions\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "def format_docs(docs):\n",
        "    \"\"\"Format retrieved documents for the prompt.\"\"\"\n",
        "    return \"\\n\\n\".join([\n",
        "        f\"[From {doc.metadata.get('source', 'Unknown')}]\\n{doc.page_content}\"\n",
        "        for doc in docs\n",
        "    ])\n",
        "\n",
        "# Create the RAG chain using LCEL\n",
        "rag_chain = \"INSERT DEFINITION HERE\"\n",
        "\n",
        "# Wrap in a class to match the expected interface\n",
        "class RAGChain:\n",
        "    \"\"\"Wrapper class to provide RetrievalQA-like interface.\"\"\"\n",
        "    def __init__(self, chain, retriever):\n",
        "        self.chain = chain\n",
        "        self.retriever = retriever\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"Execute RAG query and return result with source documents.\"\"\"\n",
        "        query = inputs[\"query\"]\n",
        "        # Get relevant documents - use invoke for newer LangChain versions\n",
        "        try:\n",
        "            # Try the newer invoke method first\n",
        "            docs = self.retriever.invoke(query)\n",
        "        except AttributeError:\n",
        "            # Fallback to get_relevant_documents for older versions\n",
        "            docs = self.retriever.get_relevant_documents(query)\n",
        "        # Run the chain\n",
        "        answer = self.chain.invoke(query)\n",
        "        return {\n",
        "            \"result\": answer,\n",
        "            \"source_documents\": docs\n",
        "        }\n",
        "\n",
        "qa_chain = RAGChain(rag_chain, retriever)\n",
        "\n",
        "print(\"RAG chain created and ready to use\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwRwWDPOmoAk"
      },
      "source": [
        "## Step 6: Test Retrieval\n",
        "\n",
        "This step validates that the retrieval system is working correctly before generating answers:\n",
        "\n",
        "- **Define test query**: Create a sample question to evaluate the retrieval functionality\n",
        "- **Execute retrieval**: Query the vector store to find the most semantically similar document chunks\n",
        "- **Display results**: Show the retrieved documents with their source files and content previews\n",
        "- **Verify relevance**: Check that the retrieved chunks are actually relevant to the test query\n",
        "- **Confirm functionality**: Ensure the retrieval system is finding appropriate context for questions\n",
        "\n",
        "Follow along with the workshop and try out different test queries based off the data in the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbnyCAySmoAk"
      },
      "outputs": [],
      "source": [
        "# Test retrieval to see what documents are found\n",
        "test_query = \"What are Monte Carlo methods?\"\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Use invoke for newer LangChain versions, fallback to get_relevant_documents\n",
        "try:\n",
        "    retrieved_docs = retriever.invoke(test_query)\n",
        "except AttributeError:\n",
        "    retrieved_docs = retriever.get_relevant_documents(test_query)\n",
        "\n",
        "print(f\"Retrieved {len(retrieved_docs)} documents for query: '{test_query}'\\n\")\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    source = doc.metadata.get('source', 'Unknown')\n",
        "    print(f\"{i}. From: {source}\")\n",
        "    print(f\"   Content preview: {doc.page_content[:500]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziAqKkHsmoAk"
      },
      "source": [
        "## Step 7: RAG Query Function\n",
        "\n",
        "This step creates a user-friendly function to query the RAG system:\n",
        "\n",
        "- **Define query function**: Create a `rag_query()` function that handles the complete RAG workflow\n",
        "- **Retrieve context**: Automatically fetch relevant document chunks based on the user's question\n",
        "- **Generate answer**: Use the LLM to synthesize an answer from the retrieved context\n",
        "- **Display results**: Present the question, answer, and optionally the source documents used\n",
        "- **Format output**: Organize the response in a readable format with clear separation between question, answer, and sources\n",
        "- **Test functionality**: Run a sample query to demonstrate the complete RAG system in action\n",
        "\n",
        "Follow along with the workshop and try out different test queries based off the data in the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VghW8QitmoAk"
      },
      "outputs": [],
      "source": [
        "def rag_query(query: str, show_sources: bool = True):\n",
        "    \"\"\"\n",
        "    Perform a RAG query: retrieve relevant context and generate answer using Gemini.\n",
        "\n",
        "    Args:\n",
        "        query: The user's question\n",
        "        show_sources: Whether to show source documents\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'result' and 'source_documents'\n",
        "    \"\"\"\n",
        "    result = qa_chain({\"query\": query})\n",
        "\n",
        "    if show_sources:\n",
        "        print(f\"Question: {query}\\n\")\n",
        "        print(f\"Answer:\\n{result['result']}\\n\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Source Documents:\")\n",
        "        for i, doc in enumerate(result['source_documents'], 1):\n",
        "            source = doc.metadata.get('source', 'Unknown')\n",
        "            print(f\"\\n{i}. Source: {source}\")\n",
        "            print(f\"   Preview: {doc.page_content[:150]}...\")\n",
        "    else:\n",
        "        print(f\"Q: {query}\\n\")\n",
        "        print(f\"A: {result['result']}\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# Test RAG query\n",
        "test_question = \"What are the key points discussed in the documents?\"\n",
        "rag_query(test_question)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD5wi2aMmoAk"
      },
      "source": [
        "## Step 8: Interactive Query Interface\n",
        "\n",
        "This step creates an interactive command-line interface for continuous querying:\n",
        "\n",
        "- **Create interactive loop**: Build a function that allows users to ask multiple questions in a session\n",
        "- **Handle user input**: Accept questions from the user via command-line input\n",
        "- **Process queries**: Execute RAG queries for each user question and display results\n",
        "- **Manage session**: Provide commands to exit the interactive mode (quit/exit/q)\n",
        "- **Error handling**: Catch and display any errors that occur during query processing\n",
        "- **User experience**: Provide clear instructions and formatting for a smooth interaction experience\n",
        "\n",
        "Run the cell, ask questions, and quit when done!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slhL0eRomoAk"
      },
      "outputs": [],
      "source": [
        "def interactive_rag():\n",
        "    \"\"\"\n",
        "    Interactive function to ask questions and get RAG-based answers.\n",
        "    \"\"\"\n",
        "    print(\"RAG System Ready! Ask questions about the documents.\")\n",
        "    print(\"Type 'quit' or 'exit' to stop.\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Your question: \").strip()\n",
        "\n",
        "        if query.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not query:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            result = rag_query(query, show_sources=False)\n",
        "            print(\"-\" * 80 + \"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\\n\")\n",
        "\n",
        "\n",
        "interactive_rag()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}